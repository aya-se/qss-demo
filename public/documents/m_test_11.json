{
  "meeting_id": "m_test_11",
  "domain": "academic",
  "meeting_transcripts": [
    "Professor B: OK .",
    "PhD C: Oh , I don't",
    "PhD A: I think I 'm zero .",
    "Professor B: Wow ! Unprecedented .",
    "PhD C: Hello , hello , hello , hello .",
    "PhD E: Ah",
    "Grad F: Wh - what causes the crash ?",
    "PhD A: Did you fix something ?",
    "PhD C: Hello .",
    "PhD E: Five , five .",
    "PhD C: Hello , hello .",
    "Grad F: Oh , maybe it 's the turning  turning off and turning on of the mike , right ?",
    "Professor B: Uh , you think that 's you ? Oh .",
    "PhD C: Aaa - aaa - aaa .",
    "Grad F: Yeah , OK , mine 's working .",
    "PhD C: OK . That 's me .",
    "Professor B: OK . OK . So , um I guess we are  um  gonna do the digits at the end . Uh",
    "PhD D: Channel  channel three , yeah .",
    "PhD C: Channel two .",
    "PhD D: OK .",
    "PhD E: Mmm , channel five ? Doesn't work ?",
    "Professor B: Yeah , that 's the mike number there , uh  Uh , mike number five , and  channel  channel four .",
    "PhD C: Two .",
    "PhD A: Is it written on her sheet , I believe .",
    "PhD E: No ? Ah ,",
    "PhD D: Mike four .",
    "Grad F: Watch this .",
    "PhD E: era el cuatro .",
    "Grad F: Yep , that 's me .",
    "PhD E: Yeah .",
    "PhD A: But , channel",
    "PhD E: Yeah yeah yeah .",
    "Professor B: This is you .",
    "PhD E: OK . I saw that . Ah  yeah , it 's OK .",
    "Professor B: Yeah . And I 'm channel uh two I think ,",
    "PhD C: Ooo .",
    "Professor B: or channel",
    "PhD C: I think I 'm channel two .",
    "Professor B: Oh , I 'm channel  must be channel one . Channel one ?",
    "PhD E: Channel   I decided to talk about that .",
    "Professor B: Yes , OK . OK . So uh  I also copied uh the results that we all got in the mail I think from uh   from OGI and we 'll go  go through them also . So where are we on   on uh   our runs ?",
    "PhD D: Uh so .  uh  We  So  As I was already said , we  we mainly focused on uh four kind of features .",
    "Professor B: Excuse me .",
    "PhD D: The PLP , the PLP with JRASTA , the MSG , and the MFCC from the baseline Aurora .",
    "Professor B: Mm - hmm .",
    "PhD D: Uh , and we focused for the  the test part on the English and the Italian . Um . We 've trained uh several neural networks on  so  on the TI - digits English  and on the Italian data and also on the broad uh  English uh French and uh Spanish databases . Mmm , so there 's our result tables here , for the tandem approach , and um , actually what we  we @ @ observed is that if the network is trained on the task data it works pretty well .",
    "Professor B: OK . Our  our uh   There 's a   We 're pausing for a photo",
    "PhD C: Chicken on the grill . Try that corner .",
    "PhD A: How about over th from the front of the room ?",
    "PhD C: Yeah , it 's longer .",
    "Professor B: We 're pausing for a photo opportunity here . Uh .  Uh . So .",
    "Grad F: Oh wait wait wait wait wait . Wait .",
    "PhD C: Get out of the  Yeah .",
    "Grad F: Hold on . Hold on .",
    "Professor B: OK .",
    "Grad F: Let me give you a black screen .",
    "Professor B: He 's facing this way . What ? OK , this  this would be a  good section for our silence detection .",
    "Grad F: OK .",
    "PhD C: Mm - hmm .",
    "Professor B: Um Oh .",
    "Grad F: Musical chairs everybody !",
    "Professor B: OK . So um ,  you were saying  about the training data  Yeah .",
    "PhD D: Yeah , so if the network is trained on the task data um  tandem works pretty well . And uh actually we have uh , results are similar Only on ,",
    "PhD A: Do you mean if it 's trained only on  On data from just that task ,",
    "PhD D: yeah .",
    "PhD A: that language ?",
    "PhD D: Just that task . But actually we didn't train network on  uh both types of data I mean  uh  phonetically ba phonetically balanced uh data and task data .",
    "PhD A: Mmm .",
    "PhD D: We only did either task  task data or  uh broad  data .",
    "PhD A: Mm - hmm .",
    "PhD D: Um  Yeah . So ,",
    "Professor B: So how  I mean  clearly it 's gonna be good then",
    "PhD A: So what 's th",
    "Professor B: but the question is how much  worse is it  if you have broad data ? I mean ,  my assump From what I saw from the earlier results , uh I guess last week ,  was that um ,  if you  trained on one language and tested on another , say , that  the results were  were relatively poor .",
    "PhD D: Mmm . Yeah .",
    "Professor B: But  but the question is if you train on one language  but you have a broad coverage  and then test in another ,  does that   is that improve things  i c in comparison ?",
    "PhD D: If we use the same language ?",
    "Professor B: No , no , no . Different lang So  um  If you train on TI - digits  and test on Italian digits ,  you do poorly ,  let 's say .",
    "PhD D: Mm - hmm .",
    "Professor B: I don't have the numbers in front of me ,",
    "PhD D: But  Yeah but I did not uh do that .",
    "Professor B: so I 'm just imagining . E So , you didn't train on  TIMIT and test on   on Italian digits , say ?",
    "PhD D: We  No , we did four  four kind of  of testing , actually . The first testing is  with task data  So , with nets trained on task data . So for Italian on the Italian speech @ @ . The second test is trained on a single language um with broad database , but the same language as the t task data .",
    "Professor B: OK .",
    "PhD D: But for Italian we choose Spanish which  we assume is close to Italian . The third test is by using , um the three language database",
    "Professor B: W which in",
    "PhD D: and the fourth is",
    "Professor B: It has three languages . That 's including the w the   the",
    "PhD D: This includes",
    "Professor B: the one that it 's",
    "PhD D: Yeah .",
    "PhD A: In",
    "PhD D: But  not digits . I mean it 's",
    "PhD A: The three languages  is not digits ,",
    "Professor B: Right .",
    "PhD A: it 's the broad  data . OK .",
    "PhD D: Yeah And the fourth test is uh  excluding from these three languages the language  that is  the task language .",
    "Professor B: Oh , OK , yeah , so , that is what I wanted to know .",
    "PhD D: Yeah .",
    "Professor B: I just wasn't saying it very well , I guess .",
    "PhD D: Uh , yeah . So um  for uh TI - digits for ins example  uh when we go from TI - digits training to  TIMIT training  uh we lose  uh around ten percent , uh . The error rate increase u of  of  of ten percent , relative .",
    "Professor B: Relative . Right .",
    "PhD D: So this is not so bad . And then when we jump to the multilingual data it 's uh it become worse and , well Around uh , let 's say ,  twenty perc twenty percent further .",
    "Professor B: Ab - about how much ?",
    "PhD D: So . Yeah .",
    "Professor B: Twenty percent further ?",
    "PhD D: Twenty to  to thirty percent further . Yeah .",
    "PhD A: And so , remind me , the multilingual stuff is just the broad data . Right ? It 's not the digits .",
    "PhD D: Yeah .",
    "PhD A: So it 's the combination of  two things there . It 's  removing the  task specific  training and  it 's adding other languages .",
    "PhD D: Yeah . Yeah .",
    "PhD A: OK .",
    "PhD D: But the first step is al already removing the task s specific from  from",
    "PhD A: Already , right right right .",
    "PhD D: So .",
    "PhD A: So they were sort of building  here ?",
    "PhD D: And we lose",
    "PhD A: OK ?",
    "PhD D: Yeah . Uh  So , basically when it 's trained on the  the multilingual broad data  um or number  so , the  the  ratio of our error rates uh with the  baseline error rate is around  uh one point one .",
    "Professor B: Yes .  And it 's something like one point three of  of the  uh",
    "PhD D: So .",
    "Professor B: I i if you compare everything to the first case at the baseline , you get something like one point one for the  for the using the same language but a different task , and something like one point three  for three  three languages  broad stuff .",
    "PhD D: No no no . Uh same language we are at uh  for at English at O point eight . So it improves ,  compared to the baseline . But  So . Le - let me .",
    "Professor B: I  I  I 'm sorry .",
    "PhD D: Tas - task data",
    "Professor B: I  I  I meant something different by baseline",
    "PhD D: we are u Yeah .",
    "Professor B: So let me  let me  Um ,  so ,  um",
    "PhD D: Mmm .",
    "Professor B: OK , fine . Let 's  let 's use the conventional meaning of baseline .",
    "PhD D: Hmm .",
    "Professor B: I  I  By baseline here I meant  uh using the task specific data .",
    "PhD D: Oh yeah , the f Yeah , OK .",
    "Professor B: But uh   uh , because that 's what you were just doing with this ten percent .",
    "PhD D: Yeah .",
    "Professor B: So I was just  I just trying to understand that .",
    "PhD D: Yeah . Sure .",
    "Professor B: So if we call  a factor of w just one , just normalized to one , the word error rate  that you have  for using TI - digits as  as  training and TI - digits as test ,",
    "PhD D: Mmm .",
    "Professor B: uh different words , I 'm sure ,",
    "PhD D: Mm - hmm .",
    "Professor B: but   but uh , uh the same  task and so on .",
    "PhD D: Mm - hmm .",
    "Professor B: If we call that \" one \" ,  then what you 're saying is  that the word error rate  for the same language but using  uh different training data than you 're testing on , say TIMIT and so forth ,  it 's one point one .",
    "PhD D: Mm - hmm . Yeah , it 's around one point one .",
    "Professor B: Right . And if it 's",
    "PhD D: Yeah .",
    "Professor B: you  do  go to  three languages including the English ,  it 's something like one point three . That 's what you were just saying , I think .",
    "PhD D: Ye Uh , more actually .",
    "PhD A: One point four ?",
    "PhD D: If I  Yeah .",
    "PhD A: So , it 's an additional thirty percent .",
    "PhD D: What would you say ? Around one point four",
    "Professor B: OK .",
    "PhD D: yeah .",
    "Professor B: And if you exclude  English ,  from this combination , what 's that ?",
    "PhD D: If we exclude English ,  um  there is  not much difference with the  data with English .",
    "Professor B: Aha !",
    "PhD D: So . Yeah .",
    "Professor B: That 's interesting .  That 's interesting . Do you see ? Because  Uh ,",
    "PhD D: Uh .",
    "Professor B: so  No , that  that 's important . So what  what it 's saying here is just that \" yes , there is a reduction  in performance ,  when you don't  um  have the s  when you don't have  um",
    "PhD A: Task data .",
    "Professor B: Wait a minute , th th the",
    "PhD D: Hmm .",
    "Professor B: No , actually  it 's interesting . So it 's  So when you go to a different task , there 's actually not so  different . It 's when you went to these  So what 's the difference between two and three ? Between the one point one case and the one point four case ? I 'm confused .",
    "PhD A: It 's multilingual .",
    "PhD D: Yeah . The only difference it 's  is that it 's multilingual  Um",
    "Professor B: Cuz in both  in both  both of those cases , you don't have the same task .",
    "PhD D: Yeah . Yeah sure .",
    "Professor B: So is  is the training data for the  for this one point four case  does it include the training data for the one point one case ?",
    "PhD D: Uh yeah .",
    "Grad F: Yeah , a fraction of it .",
    "PhD D: A part of it , yeah .",
    "Professor B: How m how much bigger is it ?",
    "PhD D: Um  It 's two times ,",
    "Grad F: Yeah , um .",
    "PhD D: actually ? Yeah . Um . The English data   No , the multilingual databases are two times the  broad English  data . We just wanted to keep this , w well , not too huge . So .",
    "Professor B: So it 's two times , but it includes the  but it includes the broad English data .",
    "PhD D: I think so . Do you  Uh , Yeah .",
    "Professor B: And the broad English data is what you got this one point one  with . So that 's TIMIT basically right ?",
    "PhD D: Yeah .",
    "Grad F: Mm - hmm .",
    "Professor B: So it 's band - limited TIMIT . This is all eight kilohertz sampling .",
    "PhD D: Mm - hmm .",
    "Grad F: Mm - hmm .",
    "PhD D: Yeah .",
    "Grad F: Downs Right .",
    "Professor B: So you have band - limited TIMIT ,  gave you uh almost as good as a result as using TI - digits  on a TI - digits test . OK ?",
    "PhD D: Hmm ?",
    "Professor B: Um  and  um But ,  when you add in more training data but keep the neural net the same size ,  it  um performs worse on the TI - digits . OK , now all of this is   This is noisy  TI - digits , I assume ? Both training and test ?",
    "Professor B: Yeah . OK . Um OK . Well .  We  we  we may just need to uh  So I mean it 's interesting that h going to a different  different task didn't seem to hurt us that much , and going to a different language um It doesn't seem to matter  The difference between three and four is not particularly great , so that means that  whether you have the language in or not is not such a big deal .",
    "PhD D: Mmm .",
    "Professor B: It sounds like um  uh  we may need to have more  of uh things that are similar to a target language or  I mean .  You have the same number of parameters in the neural net , you haven't increased the size of the neural net , and maybe there 's just   just not enough  complexity to it to represent  the variab increased variability in the  in the training set . That  that could be . Um  So , what about  So these are results with  uh th  that you 're describing now , that  they are pretty similar for the different features or   or uh",
    "PhD D: Uh , let me check . Uh .",
    "Professor B: Yeah .",
    "PhD D: So . This was for the PLP ,",
    "Professor B: Yeah .",
    "PhD D: Um . The  Yeah . For the PLP with JRASTA the   the  we  This is quite the same  tendency ,  with a slight increase of the error rate ,  uh if we go to  to TIMIT . And then it 's  it gets worse with the multilingual . Um . Yeah . There  there is a difference actually with  b between PLP and JRASTA is that  JRASTA  seems to  perform better with the highly mismatched  condition  but slightly  slightly worse  for the well matched condition . Mmm .",
    "Professor B: I have a suggestion , actually , even though it 'll delay us slightly , would  would you mind  running into the other room and making  copies of this ? Cuz we 're all sort of  If we c if we could look at it , while we 're talking , I think it 'd be",
    "PhD D: Yeah , yeah . OK .",
    "Professor B: uh   Uh , I 'll  I 'll sing a song or dance or something while you  do it , too .",
    "PhD A: So um",
    "Grad F: Alright .",
    "PhD A: Go ahead . Ah , while you 're gone I 'll ask s some of my questions .",
    "Professor B: Yeah .",
    "PhD A: Um .",
    "Professor B: Yeah . Uh , this way and just slightly to the left , yeah .",
    "PhD A: The um  What was  Was this number  forty or  It was roughly the same as this one ,  he said ? When you had the two language versus the three language ?",
    "Professor B: Um . That 's what he was saying .",
    "PhD A: That 's where he removed English ,",
    "Grad F: Yeah .",
    "PhD A: right ?",
    "Professor B: Right .",
    "Grad F: It sometimes , actually , depends on what features you 're using .",
    "Professor B: Yeah . But  but i it sounds like",
    "Grad F: Um , but    He  Mm - hmm .",
    "Professor B: I mean . That 's interesting because  it  it seems like what it 's saying is not so much that you got hurt  uh because  you  uh didn't have so much representation of English , because in the other case you don't get hurt any more , at least when  it seemed like uh it  it might simply be a case that you have something that is just much more diverse ,",
    "PhD A: Mm - hmm .",
    "Professor B: but you have the same number of parameters representing it .",
    "PhD A: Mm - hmm . I wonder  were um all three of these nets  using the same output ? This multi - language  uh labelling ?",
    "Grad F: He was using uh sixty - four phonemes from  SAMPA .",
    "PhD A: OK , OK .",
    "Grad F: Yeah .",
    "PhD A: So this would   From this you would say , \" well , it doesn't really matter if we put Finnish  into  the training of the neural net ,  if there 's  gonna be ,  you know , Finnish in the test data . \" Right ?",
    "Professor B: Well , it 's  it sounds   I mean , we have to be careful , cuz we haven't gotten a good result yet .",
    "PhD A: Yeah .",
    "Professor B: And comparing different bad results can be  tricky .",
    "PhD A: Hmm .",
    "Professor B: But I  I  I   I think it does suggest that it 's not so much uh  uh cross  language as cross type of speech .",
    "PhD A: Mm - hmm .",
    "Professor B: It 's  it 's um   But we did  Oh yeah , the other thing I was asking him , though , is that I think that in the case  Yeah , you  you do have to be careful because of com compounded results . I think we got some earlier results  in which you trained on one language and tested on another and you didn't have  three , but you just had one  language . So you trained on  one type of digits and tested on another . Didn - Wasn't there something of that ? Where you ,  say , trained on Spanish and tested on  on TI - digits , or the other way around ? Something like that ?",
    "PhD E: No .",
    "Professor B: I thought there was something like that ,  that he showed me  last week . We 'll have to wait till we get",
    "PhD A: Yeah , that would be interesting .",
    "Professor B: Um , This may have been what I was asking before , Stephane , but   but , um , wasn't there something that you did ,  where you trained  on one language and tested on another ? I mean no  no mixture but just",
    "Grad F: I 'll get it for you .",
    "PhD D: Uh , no , no .",
    "Professor B: We 've never just trained on one lang",
    "PhD D: Training on a single language , you mean , and testing on the other one ?",
    "Professor B: Yeah .",
    "PhD D: Uh , no .",
    "PhD E: Not yet .",
    "PhD D: So the only  task that 's similar to this is the training on two languages , and  that",
    "Professor B: But we 've done a bunch of things where we just trained on one language . Right ? I mean , you haven't  you haven't done all your tests on multiple languages .",
    "PhD D: Uh , No . Either thi this is test with  uh the same language  but from the broad data , or it 's test with  uh different languages also from the broad data , excluding the  So , it 's  it 's three or  three and four .",
    "PhD E: The early experiment that",
    "PhD A: Did you do different languages from digits ?",
    "PhD D: Uh . No . You mean  training digits  on one language and using the net  to recognize on the other ?",
    "PhD A: Digits on another language ?",
    "PhD D: No .",
    "Professor B: See , I thought you showed me something like that last week . You had a  you had a little",
    "PhD D: Uh ,  No , I don't think so .",
    "Professor B: Um What",
    "PhD C: These numbers are uh  ratio to baseline ?",
    "Professor B: So , I mean wha what 's the",
    "PhD D: So .",
    "Professor B: This  this chart  this table that we 're looking at  is um , show is all testing for TI - digits , or  ?",
    "Grad F: Bigger is worse .",
    "PhD D: So you have uh basically two  uh parts .",
    "Grad F: This is error rate , I think .",
    "PhD C: Ratio .",
    "Grad F: No .  No .",
    "PhD D: The upper part is for TI - digits",
    "Grad F: Yeah , yeah , yeah .",
    "PhD D: and it 's divided in three  rows  of four  four rows each .",
    "Grad F: Mm - hmm .",
    "Professor B: Yeah .",
    "PhD D: And the first four rows is well - matched , then the s the second group of four rows is mismatched , and  finally highly mismatched . And then the lower part is for Italian and it 's the same   the same thing .",
    "PhD A: So , so the upper part is training  TI - digits ?",
    "PhD D: So . It 's  it 's the HTK results , I mean . So it 's  HTK training testings  with different kind of features",
    "PhD A: Ah .",
    "PhD D: and what appears in the  uh left column is  the networks that are used for doing this .",
    "Professor B: Hmm .",
    "PhD D: So . Uh Yeah .",
    "Professor B: Well , What was is that i What was it that you had  done  last week when you showed  Do you remember ? Wh - when you showed me  the  your table last week ?",
    "PhD D: It - It was part of these results . Mmm . Mmm .",
    "PhD A: So where is the baseline  for the TI - digits  located in here ?",
    "PhD D: You mean the HTK Aurora baseline ?",
    "PhD A: Yeah .",
    "PhD D: It 's uh the one hundred number . It 's , well , all these numbers are the ratio  with respect to the baseline .",
    "PhD A: Ah ! Ah , OK , OK .",
    "Professor B: So this is word  word error rate , so a high number is bad .",
    "PhD D: Yeah , this is  a word error rate ratio .",
    "PhD E: Yeah .",
    "PhD A: OK , I see .",
    "PhD D: Yeah . So , seventy point two means that  we reduced the error rate uh by thirty  thirty percent .",
    "PhD A: OK , OK , gotcha .",
    "PhD D: So .",
    "Professor B: OK ,  so if we take",
    "PhD D: Hmm .",
    "Professor B: uh um let 's see PLP  uh with on - line  normalization and  delta - del so that 's this thing you have circled here  in the second column ,",
    "PhD D: Yeah .",
    "Professor B: um  and \" multi - English \" refers to what ?",
    "PhD D: To TIMIT . Mmm . Then you have  uh MF ,  MS and ME which are for French , Spanish and English . And , yeah . Actually I   I uh forgot to say that  the multilingual net are trained  on  uh  features without the s derivatives uh but with  increased frame numbers . Mmm . And we can  we can see on the first line of the table that it  it   it 's slightly  slightly worse when we don't use delta but it 's not   not that much .",
    "Professor B: Right . So w w So , I 'm sorry . I missed that . What 's MF , MS and ME ?",
    "PhD A: Multi - French , Multi - Spanish",
    "PhD D: So . Multi - French , Multi - Spanish , and Multi - English .",
    "Professor B: Uh OK . So , it 's  uh  broader vocabulary . Then  And",
    "PhD D: Yeah .",
    "Professor B: OK so I think what I 'm  what I saw in your smaller chart that I was thinking of was  was  there were some numbers I saw , I think , that included these multiple languages and it  and I was seeing  that it got worse . I  I think that was all it was . You had some very limited results that  at that point",
    "PhD D: Yeah .",
    "Professor B: which showed  having in these  these other languages . In fact it might have been just this last category ,  having two languages broad that were  where  where English was removed . So that was cross language and the  and the result was quite poor . What I   we hadn't seen yet was that if you added in the English , it 's still poor .",
    "PhD D: Yeah .",
    "Professor B: Uh   Um now , what 's the noise condition  um  of the training data",
    "PhD D: Still poor .",
    "Professor B: Well , I think this is what you were explaining . The noise condition is the same  It 's the same uh Aurora noises uh , in all these cases  for the training .",
    "PhD D: Yeah . Yeah .",
    "Professor B: So there 's not a  statistical  sta a strong st  statistically different  noise characteristic between  uh the training and test",
    "PhD D: No these are the s s s same noises ,",
    "Professor B: and yet we 're seeing some kind of effect",
    "PhD D: yeah . At least  at least for the first   for the well - matched ,",
    "Grad F: Well matched condition .",
    "Professor B: Right .",
    "PhD D: yeah .",
    "Professor B: So there 's some kind of a  a  an effect from having these  uh this broader coverage um Now I guess what we should try doing with this is try  testing these on u this same sort of thing on  you probably must have this  lined up to do . To try the same t  with the exact same training , do testing on  the other languages .",
    "PhD D: Mmm .",
    "Professor B: On  on um  So . Um , oh I well , wait a minute . You have this here , for the Italian . That 's right . OK , so ,  So .",
    "PhD D: Yeah . Yeah , so for the Italian the results are  uh  stranger um  Mmm . So what appears is that perhaps Spanish is  not very close to Italian because uh , well ,  when using the  the network trained only on Spanish it 's   the error rate is  almost uh twice  the baseline error rate .",
    "Professor B: Mm - hmm .",
    "PhD D: Mmm .  Uh .",
    "Professor B: Well , I mean , let 's see . Is there any difference in  So it 's in  the uh  So you 're saying that  when you train on English  and  uh  and  and test on",
    "PhD D: Yeah .",
    "Professor B: No , you don't have training on English testing",
    "PhD D: There  there is  another difference , is that the noise  the noises are different .",
    "Professor B: In  in what ?",
    "PhD D: Well , For  for the Italian part I mean the  uh  the um  networks are trained with noise from  Aurora  TI - digits ,",
    "PhD E: Aurora - two .",
    "PhD D: mmm .",
    "Professor B: And the noise is different in th",
    "PhD D: Yeah . And perhaps the noise are  quite different from the noises  in the speech that Italian .",
    "Professor B: Do we have any um  test sets  uh in  any other language that um have the same noise as in  the Aurora ?",
    "PhD D: And",
    "PhD E: Mmm , no .",
    "PhD D: No .",
    "PhD A: Can I ask something real quick ? In  in the upper part   in the English  stuff ,  it looks like the very best number is sixty point nine ? and that 's in the uh   the third  section in the upper part under PLP JRASTA , sort of the middle column ?",
    "PhD D: Yeah .",
    "PhD A: I is that  a noisy condition ?",
    "PhD D: Yeah .",
    "PhD A: So that 's matched training ? Is that what that is ?",
    "PhD D: It 's  no , the third part , so it 's uh  highly mismatched . So . Training and  test noise are different .",
    "PhD A: So  why do you get your best number in  Wouldn't you get your best number in the clean case ?",
    "PhD C: Well , it 's relative to the um  baseline mismatching",
    "PhD D: Yeah .",
    "PhD A: Ah ,",
    "PhD D: Yeah . Yeah .",
    "PhD A: OK so these are not  OK , alright , I see .",
    "PhD C: Yeah .",
    "PhD A: OK . And then  so , in the  in the um   in the  non - mismatched clean case ,  your best one was under MFCC ? That sixty - one point four ?",
    "PhD D: Yeah .  But it 's not a clean case . It 's  a noisy case but  uh training and test noises are the same .",
    "PhD A: Oh ! So this upper third ?",
    "PhD D: So  Yeah .",
    "PhD A: Uh that 's still noisy ?",
    "PhD D: Yeah .",
    "PhD A: Ah , OK .",
    "PhD D: So it 's always noisy basically ,",
    "PhD A: Mm - hmm .",
    "PhD D: and ,  well , the",
    "PhD A: I see .",
    "PhD D: Mmm .",
    "Professor B: OK ? Um  So uh , I think this will take some  looking at , thinking about . But ,  what is uh  what is currently running , that 's  uh , i that  just filling in the holes here or  or  ?   pretty much ?",
    "PhD D: Uh , no we don't plan to fill the holes",
    "Professor B: OK .",
    "PhD D: but  actually there is something important , is that  um we made a lot of assumption concerning the on - line normalization and we just noticed  uh recently that  uh the  approach that we were using  was not  uh  leading to very good results  when we  used the straight features to HTK . Um   Mmm . So basically d  if you look at the  at the left of the table ,  the first uh row ,  with eighty - six , one hundred , and forty - three and seventy - five , these are the results we obtained for Italian  uh with  straight  mmm , PLP features  using on - line normalization .",
    "Professor B: Mm - hmm .",
    "PhD D: Mmm . And the , mmm  what 's  in the table , just  at the left of the PLP twelve  on - line normalization column , so , the numbers seventy - nine , fifty - four and  uh forty - two  are the results obtained by uh Pratibha with  uh his on - line normalization  uh her on - line normalization approach .",
    "PhD A: Where is that ? seventy - nine , fifty",
    "Professor B: Uh , it 's just sort of sitting right on the uh  the column line .",
    "PhD D: So .",
    "PhD E: Fifty - one ? This",
    "PhD A: Oh I see , OK .",
    "Professor B: Uh .  Yeah .",
    "PhD D: Just  uh Yeah . So these are the results of  OGI with  on - line normalization and straight features to HTK . And the previous result , eighty - six and so on ,  are with our  features straight to HTK .",
    "Professor B: Yes . Yes .",
    "PhD D: So  what we see that  is  there is that um  uh the way we were doing this was not correct , but  still  the networks  are very good . When we use the networks  our number are better that  uh Pratibha results .",
    "PhD E: We improve .",
    "Professor B: So , do you know what was wrong with the on - line normalization , or  ?",
    "PhD D: Yeah . There were diff there were different things and  basically ,  the first thing is the mmm ,  alpha uh  value . So , the recursion  uh  part . um ,  I used point five percent ,  which was the default value in the   in the programs here . And Pratibha used five percent .",
    "Professor B: Uh",
    "PhD D: So it adapts more  quickly",
    "Professor B: Yes . Yeah .",
    "PhD D: Um , but , yeah . I assume that this was not important because  uh previous results from  from Dan and  show that basically  the  both  both values g give the same  same  uh results . It was true on uh  TI - digits but it 's not true on Italian .",
    "Professor B: Mm - hmm .",
    "PhD D: Uh , second thing is the initialization of the  stuff . Actually ,  uh what we were doing is to start the recursion from the beginning of the  utterance . And using initial values that are the global mean and variances  measured across the whole database .",
    "Professor B: Right . Right .",
    "PhD D: And Pratibha did something different is that he  uh she initialed the um values of the mean and variance  by computing  this on the  twenty - five first frames of each utterance . Mmm . There were other minor differences , the fact that  she used fifteen dissities instead s instead of thirteen , and that she used C - zero instead of log energy . Uh , but the main differences concerns the recursion . So .  Uh , I changed the code uh and now we have a baseline that 's similar to the OGI baseline .",
    "Professor B: OK .",
    "PhD D: We  It  it 's slightly  uh different because  I don't exactly initialize the same way she does . Actually I start ,  mmm , I don't wait to a fifteen  twenty - five  twenty - five frames  before computing a mean and the variance  to e to  to start the recursion .",
    "PhD C: Mm - hmm .",
    "Professor B: Yeah .",
    "PhD D: I  I use the on - line scheme and only start the re recursion after the twenty - five   twenty - fifth frame . But , well it 's similar . So  uh I retrained  the networks with  these  well , the  the  the networks are retaining with these new  features .",
    "Professor B: Mm - hmm .",
    "PhD D: And , yeah .",
    "Professor B: OK .",
    "PhD D: So basically what I expect is that  these numbers will a little bit go down but  perhaps not  not so much",
    "Professor B: Right .",
    "PhD D: because  I think the neural networks learn perhaps  to",
    "Professor B: Right .",
    "PhD D: even if the features are not  normalized . It  it will learn how to normalize and",
    "Professor B: OK , but I think that  given the pressure of time we probably want to draw  because of that  especially , we wanna draw some conclusions from this , do some reductions  in what we 're looking at ,",
    "PhD D: Yeah .",
    "Professor B: and make some strong decisions for what we 're gonna do testing on before next week . So do you  are you  w did you have something going on , on the side , with uh multi - band  or  on  on this ,",
    "PhD D: Yeah  I",
    "Professor B: or  ?",
    "PhD D: No , I  we plan to start this uh so , act actually we have discussed uh  @ @ um , these  what we could do  more as a  as a research and   and  we were thinking perhaps that  uh  the way we use the tandem is not  Uh , well , there is basically perhaps a flaw in the  in the  the stuff because  we  trained the networks  If we trained the networks on the  on  a language and a t or a specific  task ,",
    "Professor B: Mm - hmm .",
    "PhD D: um , what we ask is  to the network  is to put the bound the decision boundaries somewhere in the space .",
    "Professor B: Mmm .",
    "PhD D: And uh  mmm and ask the network to put one ,  at one side of the  for  for a particular phoneme at one side of the boundary  decision boundary and one for another phoneme at the other side . And  so there is kind of reduction of the information there that 's not correct because if we change task  and if the phonemes are not in the same context in the new task ,  obviously the  decision boundaries are not   should not be at the same  place .",
    "Professor B: I di",
    "PhD D: But the way the feature gives  The  the way the network gives the features is that it reduce completely the   it removes completely the information   a lot of information from the  the features  by uh  uh  placing the decision boundaries at  optimal places for  one kind of  data but  this is not the case for another kind of data .",
    "Professor B: It 's a trade - off ,",
    "PhD D: So",
    "Professor B: right ? Any - anyway go ahead .",
    "PhD D: Yeah . So uh what we were thinking about is perhaps  um one way  to solve this problem is increase the number of  outputs of the neural networks . Doing something like , um  um phonemes within context and , well , basically context dependent phonemes .",
    "Professor B: Maybe . I mean , I  I think  you could make  the same argument , it 'd be just as legitimate ,  for hybrid systems  as well . Right .",
    "PhD D: Yeah but , we know that",
    "Professor B: And in fact ,  th things get better with context dependent  versions . Right ?",
    "PhD D: Ye - yeah but here it 's something different . We want to have features",
    "Professor B: Yeah .",
    "PhD D: uh well ,  um .",
    "Professor B: Yeah , but it 's still true  that what you 're doing  is you 're ignoring  you 're  you 're coming up with something to represent ,  whether it 's a distribution ,  probability distribution or features , you 're coming up with a set of variables  that are representing  uh ,  things that vary w over context .",
    "PhD D: Mm - hmm .",
    "Professor B: Uh , and you 're  putting it all together , ignoring the differences in context . That  that 's true  for the hybrid system , it 's true for a tandem system . So , for that reason , when you  in  in  in a hybrid system ,  when you incorporate context one way or another ,  you do get better scores .",
    "PhD D: Yeah .",
    "Professor B: OK ? But I  it 's  it 's a big deal  to get that . I  I 'm  I 'm sort of  And once you  the other thing is that once you represent  start representing more and more context  it is  uh  much more  um specific  to a particular task in language . So um Uh , the   the acoustics associated with  uh a particular context , for instance you may have some kinds of contexts that will never occur  in one language and will occur frequently in the other , so the qu the issue of getting enough training  for a particular kind of context becomes harder . We already actually don't have a huge amount of training data um",
    "PhD D: Yeah , but  mmm , I mean ,  the  the way we  we do it now is that we have a neural network and  basically  the net network is trained almost to give binary decisions .",
    "Professor B: Right .",
    "PhD D: And  uh  binary decisions about phonemes . Nnn  Uh It 's",
    "Professor B: Almost . But I mean it  it  it does give a distribution .",
    "PhD D: Yeah .",
    "Professor B: It 's  and  and  it is true that if there 's two phones that are very similar ,  that  uh  the   i it may prefer one but it will  give a reasonably high value to the other , too .",
    "PhD D: Yeah . Yeah , sure but uh  So basically it 's almost binary decisions and  um the idea of using more  classes is  to  get something that 's  less binary decisions .",
    "Professor B: Oh no , but it would still be even more of a binary decision . It  it 'd be even more of one . Because then you would say  that in  that this phone in this context is a one ,  but the same phone in a slightly different context is a zero .",
    "PhD D: But  yeah , but",
    "Professor B: That would be even  even more distinct of a binary decision . I actually would have thought you 'd wanna go the other way and have fewer classes .",
    "PhD D: Yeah , but if",
    "Professor B: Uh , I mean for instance , the  the thing I was arguing for before , but again which I don't think we have time to try ,  is something in which you would modify the code so you could train to have several outputs on and use articulatory features",
    "PhD D: Mmm . Mm - hmm .",
    "Professor B: cuz then that would  that would go   that would be much broader and cover many different situations . But if you go to very very fine categories , it 's very  binary .",
    "PhD D: Mmm . Yeah , but I think  Yeah , perhaps you 're right , but you have more classes so  you  you have more information in your features . So ,  Um  You have more information in the  uh",
    "Professor B: Mm - hmm . True .",
    "PhD D: posteriors vector um which means that  But still the information is relevant",
    "Professor B: Mm - hmm .",
    "PhD D: because it 's  it 's information that helps to discriminate ,",
    "Professor B: Mm - hmm .",
    "PhD D: if it 's possible to be able to discriminate  among the phonemes in context .",
    "Professor B: Well it 's  it 's   it 's an interesting thought .",
    "PhD D: But the",
    "Professor B: I mean we  we could disagree about it at length",
    "PhD D: Mmm .",
    "Professor B: but the  the real thing is if you 're interested in it you 'll probably try it",
    "PhD D: Mmm .",
    "Professor B: and   and  we 'll see . But  but what I 'm more concerned with now , as an operational level , is  uh , you know ,",
    "PhD D: Mmm .",
    "Professor B: what do we do in four or five days ? Uh , and   so we have  to be concerned  with Are we gonna look at any combinations of things , you know once the nets get retrained so you have this problem out of it .",
    "PhD D: Mmm .",
    "Professor B: Um , are we going to look at  multi - band ? Are we gonna look at combinations of things ? Uh , what questions are we gonna ask , uh now that , I mean ,  we should probably turn shortly to this O G I note . Um , how are we going to  combine  with what they 've been focusing on ? Uh ,  Uh we haven't been doing any of the L D A RASTA sort of thing .",
    "PhD D: Mm - hmm .",
    "Professor B: And they , although they don't talk about it in this note , um ,  there 's um ,  the issue of the  um Mu law  business  uh  versus the logarithm , um ,  so .",
    "PhD D: Mm - hmm .",
    "Professor B: So what i what is going on right now ? What 's right  you 've got  nets retraining , Are there  is there  are there any H T K  trainings  testings going on ?",
    "PhD D: N",
    "PhD E: I  I  I 'm trying the HTK with eh ,  PLP twelve on - line delta - delta and MSG filter  together .",
    "Professor B: The combination , I see .",
    "PhD E: The combination , yeah . But I haven't result  at this moment .",
    "Professor B: MSG and  and PLP .",
    "PhD E: Yeah .",
    "Professor B: And is this with the revised  on - line normalization ?",
    "PhD E: Ye - Uh , with the old  older ,",
    "PhD D: Yeah .",
    "Professor B: Old one . So it 's using all the nets for that",
    "PhD E: yeah .",
    "Professor B: but again we have the hope that it   We have the hope that it   maybe it 's not making too much difference ,",
    "PhD E: Yeah . But  We can know soon .",
    "Professor B: but  but",
    "PhD E: Maybe .",
    "Professor B: yeah .",
    "PhD E: I don't know .",
    "PhD D: Yeah .",
    "Professor B: Uh , OK .",
    "PhD D: Uh so there is this combination , yeah . Working on combination obviously .",
    "PhD E: Mm - hmm .",
    "PhD D: Um , I will start work on multi - band . And  we  plan to work also on the idea of using both  features  and net outputs .",
    "PhD D: Um . And  we think that  with this approach perhaps  we could reduce the number of outputs of the neural network . Um , So , get simpler networks , because we still have the features . So we have um  come up with um  different kind of  broad phonetic categories . And we have  Basically we have three  types of broad phonetic classes . Well , something using place of articulation which  which leads to  nine , I think ,  broad classes . Uh , another which is based on manner , which is  is also something like nine classes . And then ,  something that combine both , and we have  twenty f  twenty - five ?",
    "Grad F: Twenty - seven .",
    "PhD D: Twenty - seven broad classes . So like , uh , oh , I don't know , like back vowels , front vowels .",
    "Professor B: So what you do  um I just wanna understand",
    "PhD D: Um For the moments we do not  don't have nets ,",
    "Professor B: so  You have two net or three nets ? Was this ? How many  how many nets do you have ? No nets .",
    "PhD D: I mean ,  It 's just  Were we just changing  the labels to retrain nets  with fewer out outputs .",
    "PhD E: Begin to work in this . We are @ @ .",
    "Professor B: Right . But  but I didn't understand",
    "PhD D: And then  Mm - hmm .",
    "Professor B: Uh .  the software currently just has  uh a  allows for I think , the one  one hot output . So you 're having multiple nets and combining them , or  ? Uh , how are you  how are you coming up with  If you say  uh  If you have a place  characteristic and a manner characteristic , how do you",
    "PhD D: It - It 's the single net ,",
    "PhD A: I think they have one output .",
    "PhD D: yeah .",
    "Professor B: Oh , it 's just one net .",
    "PhD D: It 's one net with  um  twenty - seven outputs",
    "PhD E: Yeah .",
    "Grad F: mm - hmm",
    "PhD D: if we have twenty - seven classes ,",
    "Professor B: I see . I see , OK .",
    "PhD D: yeah . So it 's  Well , it 's basically a standard net with fewer  classes .",
    "Professor B: So you 're sort of going the other way of what you were saying a bit ago instead of  yeah .",
    "PhD D: Yeah , but I think  Yeah . B b including the features , yeah .",
    "Grad F: But including the features .",
    "PhD E: Yeah .",
    "PhD D: I don't think this  will work  alone . I think it will get worse because Well , I believe the effect that  of  of too reducing too much the information is  basically  basically what happens",
    "Professor B: Uh - huh .",
    "PhD D: and",
    "Professor B: But you think if you include that  plus the other features ,",
    "PhD D: but  Yeah , because  there is perhaps one important thing that the net  brings , and OGI show showed that , is  the distinction between  sp speech and silence Because these nets are trained on well - controlled condition . I mean the labels are obtained on clean speech , and we add noise after . So this is one thing And But perhaps , something intermediary using also  some broad classes could  could bring so much more information . Uh .",
    "Professor B: So  so again then we have these broad classes and  well , somewhat broad . I mean , it 's twenty - seven instead of sixty - four ,  basically . And you have the original features .",
    "PhD D: Yeah .",
    "Professor B: Which are PLP , or something .",
    "PhD D: Yeah .",
    "Professor B: And then uh , just to remind me , all of that goes  into  uh , that all of that is transformed by uh , uh , K - KL or something , or  ?",
    "PhD D: Mm - hmm . There will probably be ,",
    "PhD E: Mu .",
    "PhD D: yeah , one single KL to transform everything",
    "Professor B: Right .",
    "PhD D: or   uh ,",
    "PhD E: No transform the PLP",
    "PhD D: per",
    "PhD E: and only transform the other I 'm not sure .",
    "Professor B: Well no ,",
    "PhD D: This is  still something  that",
    "Professor B: I think  I see .",
    "PhD D: yeah , we  don't know",
    "Professor B: So there 's a question of whether you would",
    "PhD E: Two e @ @ it 's one .",
    "PhD D: Yeah .",
    "Professor B: Right . Whether you would transform together or just one . Yeah . Might wanna try it both ways . But that 's interesting . So that 's something that you 're  you haven't trained yet but are preparing to train , and",
    "PhD D: Yeah .",
    "Professor B: Yeah . Um   Yeah , so I think Hynek will be here Monday .",
    "PhD D: Mmm .",
    "Professor B: Monday or Tuesday . So",
    "PhD D: Uh , yeah .",
    "Professor B: So I think , you know , we need to  choose the  choose the experiments carefully , so we can get uh key   key questions answered  uh before then",
    "PhD D: Mm - hmm .",
    "Professor B: and  leave other ones aside even if it  leaves incomplete  tables   someplace , uh  uh , it 's  it 's really time to   time to choose .",
    "PhD D: Mm - hmm .",
    "Professor B: Um , let me pass this out ,  by the way . Um These are  Did  did   did I interrupt you ?",
    "PhD E: Yeah , I have one .",
    "Professor B: Were there other things that you wanted to",
    "PhD D: Uh , no . I don't think so .",
    "PhD D: Yeah , I have one .",
    "Grad G: Oh , thanks .",
    "Professor B: Ah !  OK .  OK , we have  lots of them .",
    "PhD E: We have one .",
    "Professor B: OK , so  um , Something I asked  So they 're  they 're doing  the  the VAD I guess they mean voice activity detection So again , it 's the silence  So they 've just trained up a net  which has two outputs , I believe . Um  I asked uh  Hynek whether  I haven't talked to Sunil  I asked Hynek whether  they compared that to  just taking the nets we already had  and summing up the probabilities .",
    "PhD D: Mm - hmm .",
    "Professor B: Uh .  To get the speech  voice activity detection , or else just using the silence ,  if there 's only one  silence output . Um  And , he didn't think they had , um . But on the other hand , maybe they can get by with a smaller net and  maybe  sometimes you don't run the other , maybe there 's a computational advantage to having a separate net , anyway .",
    "PhD D: Mm - hmm .",
    "Professor B: So um Their uh   the results look pretty good . Um ,  I mean , not uniformly .",
    "PhD D: Yeah .",
    "Professor B: I mean , there 's a  an example or two  that you can find , where it made it slightly worse , but  uh in  in all but a couple  examples .",
    "PhD D: Mmm .",
    "Professor B: Uh .",
    "PhD E: But they have a question of the result . Um how are trained the  the LDA filter ? How obtained the LDA filter ?",
    "PhD D: Mmm .",
    "Professor B: I I 'm sorry . I don't understand your question .",
    "PhD E: Yes , um the LDA filter  needs some  training set  to obtain the filter . Maybe I don't know exactly how  they are obtained .",
    "Professor B: It 's on  training .",
    "PhD E: Training , with the training test of each  You understand me ?",
    "Professor B: No .",
    "PhD E: Yeah , uh for example ,  LDA filter  need a set of   a set of training  to obtain the filter .",
    "Professor B: Yes .",
    "PhD E: And maybe  for the Italian , for the TD  TE on for Finnish , these filter are  are obtained with their own training set .",
    "Professor B: Yes , I don't know . That 's  that 's  so that 's a  that 's a very good question , then  now that it   I understand it . It 's \" yeah , where does the LDA come from ? \" In the  In  earlier experiments , they had taken LDA  from a completely different database , right ?",
    "PhD E: Yeah . Yeah , because maybe it the same situation that the neural network training with their own",
    "PhD D: Mmm .",
    "PhD E: set .",
    "Professor B: So that 's a good question . Where does it come from ? Yeah , I don't know . Um ,  but uh to tell you the  truth , I wasn't actually looking at the LDA so much when I  I was looking at it I was  mostly thinking about the   the VAD . And um , it ap  it ap Oh what does  what does ASP ? Oh that 's",
    "PhD D: The features , yeah . Yeah .",
    "PhD E: I don't understand also",
    "Professor B: It says \" baseline ASP \" .",
    "PhD E: what is   what is the difference between ASP and uh baseline over ?",
    "PhD C: ASP .",
    "PhD D: Yeah , I don't know .",
    "PhD E: This is",
    "Professor B: Anybody know  any",
    "PhD C: Oh . There it is .",
    "Professor B: Um Cuz there 's \" baseline Aurora \"  above it .",
    "PhD C: Mm - hmm .",
    "Professor B: And it 's  This is mostly better than baseline , although in some cases it 's a little worse , in a couple cases .",
    "PhD C: Well , it says baseline ASP is twenty - three mill  minus thirteen .",
    "PhD E: Yeah .",
    "Professor B: Yeah , it says what it is . But I don't how that 's different  from",
    "PhD C: From the baseline .  OK .",
    "Professor B: I think this was   I think this is the same point we were at when  when we were up in Oregon .",
    "PhD E: Yeah .",
    "PhD D: I think   I think it 's the C - zero  using C - zero instead of log energy .",
    "PhD E: Ah , OK , mm - hmm .",
    "PhD D: Yeah , it 's this .",
    "Professor B: Oh . OK .",
    "PhD E: yeah .",
    "PhD D: It should be that , yeah .",
    "PhD A: They s they say in here that the VAD is not used as an additional feature .",
    "Professor B: Shouldn't it be",
    "PhD D: Because",
    "PhD A: Does  does anybody know how they 're using it ?",
    "Professor B: Yeah . So  so what they 're doing here is ,  i",
    "PhD D: Yeah .",
    "Professor B: if you look down at the block diagram ,  um ,  they estimate  they get a   they get an estimate  of whether it 's speech or silence ,",
    "PhD A: But that",
    "Professor B: and then they have a median filter of it .",
    "PhD A: Mm - hmm .",
    "Professor B: And so um ,  basically they 're trying to find stretches . The median filter is enforcing a  i it having some continuity .",
    "PhD A: Mm - hmm .",
    "Professor B: You find stretches where the  combination of the  frame wise VAD and the   the median filter say that there 's a stretch of silence . And then it 's going through and just throwing the data away .",
    "PhD C: Hmm .",
    "Professor B: Right ? So um",
    "PhD A: So it 's  it 's  I don't understand . You mean it 's throwing out frames ? Before",
    "Professor B: It 's throwing out chunks of frames , yeah . There 's  the  the median filter is enforcing that it 's not gonna be single cases of frames , or isolated frames .",
    "PhD A: Yeah .",
    "Professor B: So it 's throwing out frames and the thing is  um ,  what I don't understand is how they 're doing this with H T",
    "PhD A: Yeah , that 's what I was just gonna ask .",
    "Professor B: This is",
    "PhD A: How can you just throw out frames ?",
    "Professor B: Yeah . Well , you  you can ,",
    "PhD D: i",
    "Professor B: right ? I mean y you  you",
    "PhD D: Yeah .",
    "Professor B: it stretches again . For single frames I think it would be pretty hard .",
    "PhD A: Yeah .",
    "Professor B: But if you say speech starts here , speech ends there .",
    "PhD A: Mm - hmm .",
    "Professor B: Right ?",
    "PhD C: Huh .",
    "PhD D: Yeah . Yeah , you can basically remove the  the frames from the feature  feature files .",
    "Professor B: Yeah . Yeah , so I mean in the  i i in the  in the decoding , you 're saying that we 're gonna decode from here to here .",
    "PhD D: I t",
    "PhD A: Mm - hmm .",
    "Professor B: I think they 're  they 're  they 're treating it ,  you know , like uh  well , it 's not isolated word , but  but connected , you know , the  the",
    "PhD A: In the text they say that this  this is a tentative block diagram of a possible configuration we could think of . So that sort of sounds like they 're not doing that yet .",
    "Professor B: Well .  No they  they have numbers though , right ? So I think they 're  they 're doing something like that . I think that they 're  they 're  I think what I mean by tha that is they 're trying to come up with a block diagram that 's plausible for the standard . In other words , it 's  uh  I mean from the point of view of  of uh reducing the number of bits you have to transmit it 's not a bad idea to detect silence anyway .",
    "PhD A: Yeah . Yeah . I 'm just wondering what exactly did they do up in this table if it wasn't this .",
    "Professor B: Um . But it 's  the thing is it 's that  that  that 's  that 's I  I  Certainly it would be tricky about it intrans in transmitting voice ,  uh uh for listening to , is that these kinds of things  uh cut  speech off a lot .",
    "PhD A: Mm - hmm .",
    "Professor B: Right ? And so  um",
    "PhD A: Plus it 's gonna introduce delays .",
    "Professor B: It does introduce delays but they 're claiming that it 's  it 's within the   the boundaries of it .",
    "PhD A: Mmm .",
    "Professor B: And the LDA introduces delays , and b  what he 's suggesting this here is a parallel path so that it doesn't introduce  uh , any more delay . I it introduces two hundred milliseconds of delay but at the same  time the LDA  down here  I don't know  Wh what 's the difference between TLDA and SLDA ?",
    "PhD C: Temporal and spectral .",
    "Professor B: Ah , thank you .",
    "PhD E: Temporal LDA .",
    "Professor B: Yeah , you would know that .",
    "PhD C: Yeah",
    "Professor B: So um . The temporal LDA does in fact include the same  so that  I think he  well , by  by saying this is a b a tentative block di diagram I think means  if you construct it this way , this  this delay would work in that way",
    "PhD A: Ah .",
    "Professor B: and then it 'd be OK . They  they clearly did actually remove  silent sections in order  because they  got these  word error rate  results . So um I think that it 's  it 's nice to do that in this because in fact , it 's gonna give a better word error result and therefore will help within an evaluation . Whereas to whether this would actually be in a final standard , I don't know . Um . Uh , as you know , part of the problem with evaluation right now is that the  word models are pretty bad and nobody wants   has  has approached improving them . So  it 's possible that a lot of the problems  with so many insertions and so forth would go away if they were better word models  to begin with . So  this might just be a temporary thing . But  But , on the other hand , and maybe  maybe it 's a decent idea . So um The question we 're gonna wanna go  through next week when Hynek shows up I guess is given that we 've been  if you look at what we 've been trying , we 're uh looking at  uh , by then I guess , combinations of features and multi - band Uh , and we 've been looking at  cross - language , cross  task  issues . And they 've been not so much looking at  the cross task uh multiple language issues . But they 've been looking at uh   at these issues . At the on - line normalization and the uh  voice activity detection . And I guess when he comes here we 're gonna have to start deciding about  um what do we choose  from what we 've looked at  to um blend with  some group of things in what they 've looked at And once we choose that ,  how do we split up the  effort ? Uh , because we still have  even once we choose ,  we 've still got  uh another  month or so , I mean there 's holidays in the way , but  but uh  I think the evaluation data comes January thirty - first so there 's still a fair amount of time  to do things together it 's just that they probably should be somewhat more coherent between the two sites  in that  that amount of time .",
    "PhD A: When they removed the silence frames , did they insert some kind of a marker so that the recognizer knows it 's   knows when it 's time to back trace or something ?",
    "Professor B: Well , see they , I  I think they 're Um . I don't know the   the specifics of how they 're doing it . They 're   they 're getting around the way the recognizer works because they 're not allowed to  um , change the scripts  for the recognizer ,  I believe .",
    "PhD A: Oh , right . Maybe they 're just inserting some nummy frames or something ?",
    "Professor B: So . Uh . Uh , you know that 's what I had thought . But I don't  I don't think they are .",
    "PhD A: Hmm .",
    "Professor B: I mean that 's  sort of what  the way I had imagined would happen is that on the other side , yeah you p put some low level noise or something . Probably don't want all zeros .",
    "PhD A: Hmm .",
    "Professor B: Most recognizers don't like zeros but  but  you know ,  put some epsilon in or some rand",
    "PhD A: Yeah .",
    "Professor B: sorry epsilon random variable  in or something .",
    "PhD A: Some constant vector . I mean i w Or something",
    "Professor B: Maybe not a constant but it doesn't , uh  don't like to divide by the variance of that , but I mean it 's",
    "PhD A: That 's right . But something that  what I mean is something that is  very distinguishable from  speech .",
    "Professor B: Mm - hmm .",
    "PhD A: So that the  the silence model in HTK will always pick it up .",
    "Professor B: Yeah . So I  I  that 's what I thought they would do . or else , uh  uh maybe there is some indicator to tell it to start and stop , I don't know .",
    "PhD A: Hmm .",
    "Professor B: But whatever they did , I mean they have to play within the rules of this specific evaluation .",
    "PhD A: Yeah .",
    "Professor B: We c we can find out .",
    "PhD A: Cuz you gotta do something . Otherwise , if it 's just a bunch of speech , stuck together",
    "Professor B: No they 're",
    "PhD A: Yeah .",
    "Professor B: It would do badly",
    "PhD A: Yeah , right .",
    "Professor B: and it didn't so badly , right ? So they did something .",
    "PhD A: Yeah , yeah .",
    "Professor B: Yeah . Uh . So , OK , So I think  this brings me up to date a bit . It hopefully brings other  people up to date a bit . And um Um  I think  Uh , I wanna look at these numbers off - line a little bit and think about it and   and talk with everybody uh ,  outside of this meeting . Um , but uh No I mean it sounds like  I mean  there  there  there are the usual number of  of  little  little problems and bugs and so forth but it sounds like they 're getting ironed out . And now we 're  seem to be kind of in a position to actually  uh ,  look at stuff and  and  and compare things . So I think that 's  that 's pretty good . Um  I don't know what the  One of the things I wonder about ,  coming back to the first results you talked about , is  is  how much ,  uh  things could be helped  by more parameters . And uh   And uh how many more parameters we can afford to have ,   in terms of the uh computational limits . Because anyway when we go to  twice as much data  and have the same number of parameters , particularly when it 's twice as much data and it 's quite diverse , um , I wonder if having twice as many parameters would help .",
    "PhD D: Mm - hmm .",
    "Professor B: Uh , just have a bigger hidden layer . Uh But  I doubt it would  help by forty per cent . But   but uh",
    "PhD D: Yeah .",
    "Professor B: Just curious . How are we doing on the  resources ? Disk , and",
    "PhD D: I think we 're alright ,",
    "Professor B: OK .",
    "PhD D: um ,  not much problems with that .",
    "Professor B: Computation ?",
    "PhD D: It 's OK .",
    "Professor B: We",
    "PhD D: Well this table took uh  more than five days to get back .",
    "Professor B: Yeah . Yeah , well .",
    "PhD D: But  Yeah .",
    "Professor B: Are  were you folks using Gin ? That 's a  that just died , you know ?",
    "PhD D: Mmm , no . You were using Gin  perhaps , yeah ? No .",
    "PhD E: No .",
    "Professor B: No ? Oh , that 's good .",
    "Grad F: It just died .",
    "Professor B: OK . Yeah ,  we 're gonna get a replacement  server that 'll be a faster server ,  actually .",
    "PhD E: Yes .",
    "Professor B: That 'll be  It 's a  seven hundred fifty megahertz uh SUN",
    "PhD D: Hmm .  Mm - hmm .",
    "Professor B: uh  But it won't be installed for  a little while .",
    "PhD C: Tonic .",
    "Professor B: U Go ahead .",
    "Grad G: Do we  Do we have that big new IBM machine the , I think in th",
    "Professor B: We have the  little tiny IBM machine   that might someday grow up to be a big  IBM machine . It 's got s slots for eight , uh IBM was donating five , I think we only got two so far , processors . We had originally hoped we were getting eight hundred megahertz processors . They ended up being five fifty . So instead of having eight processors that were eight hundred megahertz , we ended up with two  that are five hundred and fifty megahertz . And more are supposed to come soon and there 's only a moderate amount of dat of memory . So I don't think  anybody has been sufficiently excited by it to  spend much time  uh  with it , but uh  Hopefully ,  they 'll get us some more  parts , soon and  Uh , yeah , I think that 'll be  once we get it populated ,  that 'll be a nice machine . I mean we will ultimately get eight processors in there . And uh  and uh a nice amount of memory . Uh so it 'll be a pr pretty fast Linux machine .",
    "Grad G: And if we can do things on Linux ,  some of the machines we have going already , like Swede ?",
    "Professor B: Mm - hmm .",
    "Grad G: Um It seems pretty fast .",
    "Professor B: Mm - hmm .",
    "Grad G: But  I think Fudge is pretty fast too .",
    "Professor B: Yeah , I mean you can check with uh  Dave Johnson . I mean , it  it 's   I think the machine is just sitting there . And it does have two processors , you know and   Somebody could do   you know , uh , check out  uh the multi - threading  libraries . And  I mean i it 's possible that the  I mean , I guess the prudent thing to do would be for somebody to do the work on   on getting our code running  on that machine with two processors  even though there aren't five or eight . There 's  there 's  there 's gonna be debugging hassles and then we 'd be set for when we did have five or eight , to have it really be useful . But .  Notice how I said somebody and  turned my head your direction . That 's one thing you don't get in these recordings . You don't get the   don't get the visuals but",
    "Grad G: I is it um  mostly um the neural network trainings that are  um slowing us down or the HTK runs that are slowing us down ?",
    "Professor B: Uh , I think yes . Uh ,  Isn't that right ? I mean I think you 're  you 're sort of held up by both , right ? If the  if the neural net trainings were a hundred times faster  you still wouldn't  be anything  running through these a hundred times faster because you 'd  be stuck by the HTK trainings ,",
    "PhD D: Mmm .",
    "Professor B: right ?",
    "PhD D: Yeah .",
    "Professor B: But if the HTK  I mean I think they 're both  It sounded like they were roughly equal ? Is that about right ?",
    "PhD D: Yeah .",
    "Professor B: Yeah .",
    "Grad G: Because , um  I think that 'll be running Linux , and Sw - Swede and Fudge are already running Linux so ,  um I could try to get  um the train the neural network trainings or the HTK stuff running under Linux , and to start with I 'm  wondering which one I should pick first .",
    "Professor B: Uh , probably the neural net cuz it 's probably  it  it 's   it 's um  Well , I  I don't know . They both  HTK we use for  um  this Aurora stuff Um  Um , I think  It 's not clear yet what we 're gonna use  for trainings uh  Well ,  there 's the trainings uh  is it the training that takes the time , or the decoding ? Uh , is it about equal  between the two ? For  for Aurora ?",
    "PhD D: For HTK ?",
    "Professor B: For  Yeah . For the Aurora ?",
    "PhD D: Uh Training is longer .",
    "Professor B: OK .",
    "PhD D: Yeah .",
    "Professor B: OK . Well , I don't know how we can  I don't know how to  Do we have HTK source ? Is that  Yeah .",
    "PhD D: Mmm .",
    "Professor B: You would think that would fairly trivially  the training would , anyway , th the testing  uh I don't  I don't  think would  parallelize all that well . But I think  that  you could  certainly do d um ,  distributed , sort of   Ah , no , it 's the   each individual  sentence is pretty tricky to parallelize . But you could split up the sentences in a test set .",
    "PhD A: They have a  they have a thing for doing that and th they have for awhile , in H T And you can parallelize the training .",
    "Professor B: Yeah ?",
    "PhD A: And run it on several machines",
    "Professor B: Aha !",
    "PhD A: and it just basically keeps counts . And there 's something   a final  thing that you run and it accumulates all the counts together .",
    "Professor B: I see .",
    "PhD D: Mmm .",
    "PhD A: I don't what their scripts are  set up to do for the Aurora stuff , but",
    "PhD D: Yeah .",
    "Professor B: Something that we haven't really settled on yet is other than  this Aurora stuff ,  uh what do we do , large vocabulary  training slash testing  for uh tandem systems . Cuz we hadn't really done much with tandem systems for larger stuff . Cuz we had this one collaboration with CMU and we used SPHINX . Uh , we 're also gonna be collaborating with SRI and we have their  have theirs . Um  So  I don't know Um . So I  I think the  the advantage of going with the neural net thing is that we 're gonna use the neural net trainings , no matter what , for a lot of the things we 're doing ,",
    "Grad G: OK .",
    "Professor B: whereas , w exactly which HMM  Gaussian - mixture - based HMM thing we use is gonna depend uh So with that , maybe we should uh  go to our  digit recitation task . And , it 's about eleven fifty . Canned . Uh , I can  I can start over here . Great , uh , could you give Adam a call . Tell him to He 's at two nine seven seven .",
    "Grad F: Oh .",
    "Professor B: OK . I think we can  @ @ You know Herve 's coming tomorrow , right ? Herve will be giving a talk , yeah , talk at eleven . Did uh , did everybody sign these consent Er everybody Has everyone signed a consent form before , on previous meetings ? You don't have to do it again each time Yes . microphones off"
  ],
  "topic_list": [
    {
      "topic": "Introduction to results of experiments",
      "relevant_text_span": [["40", "202"]]
    },
    {
      "topic": "Effect of training on different languages",
      "relevant_text_span": [["204", "322"]]
    },
    {
      "topic": "Effect of computational and statistical techniques on model performance",
      "relevant_text_span": [["326", "483"]]
    },
    {
      "topic": "Managing the complexity of the model",
      "relevant_text_span": [["484", "747"]]
    }
  ]
}
